# Source 配置(目前支持 csv, txt, orc, parquet, hive)
source.type=kafka
#source.path=hdfs://hacluster/user/liyihui/csv
source.path=hdfs://m7-common-cdh02:8020/user/liyihui/csv
#source.type=csv
#source.path=hdfs:///user/liyihui/csv
#source.path=hdfs://m7-common-cdh02:8020/user/liyihui/csv
## 文本(txt, csv)引入其他配置
source.firstLine.schema=false
source.line.delimiter=\n
source.field.delimiter=,
source.charset=UTF-8

#source.type=orc
#source.path=hdfs://m7-common-cdh02:8020/user/liyihui/orc
#source.path=hdfs://hacluster/user/liyihui/orc

#source.type=parquet
#source.path=hdfs:///user/liyihui/parquet

## 文本引入类型配置(默认流式)
## 根据提供的 watchType(PROCESS_CONTINUOUSLY, PROCESS_ONCE),这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据
source.watch.type=PROCESS_CONTINUOUSLY
## default 10s
source.watch.interval=60000

## Hive 引入配置(CDH)
#source.type=jdbc
source.db.type=hive
source.db.url=jdbc:hive2://172.27.128.124:10000/lyh
source.db.username=root
source.db.password=123456
source.db.driverClassName=org.apache.hive.jdbc.HiveDriver
source.db.table=csvtable2
source.db.sql=select * from csvtable2 limit 3

# sink 配置(目前只支持 kafka)
sink.kafka.version=0.11
sink.kafka.brokers=172.27.137.231:9092
sink.kafka.topic=lyh-sink

#source.type=kafka
source.kafka.version=0.11
source.kafka.brokers=172.27.137.231:9092
source.kafka.topic=lyh-source
auto.offset.reset=earliest
group.id=test2
unique.keys=id,name

## Kafka -> Kafka 相关配置
stream.sql.table=t1
stream.sql=select * from t1
stream.sql.schema=struct<id:Int,name:String>

# flink
#stream.checkpoint.enable=true
#stream.checkpoint.dir=hdfs://m7-common-cdh02:8020/tmp/csp/flink/checkpoints
#stream.checkpoint.type=fs

# hadoop
hadoop.home.dir=/root/install/config/hadoop
#hadoop.home.dir=hadoop/cdh
hadoop.user.name=work